{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Metrics: Network-Based Systemic Risk Scoring\n",
    "\n",
    "**An Interactive Tutorial**\n",
    "\n",
    "This notebook demonstrates the Matrix Metrics framework for measuring systemic risk in financial networks, based on Das (2016).\n",
    "\n",
    "## ðŸ“š What You'll Learn\n",
    "\n",
    "1. **The Problem**: Why traditional risk metrics fail\n",
    "2. **The Solution**: Network-based approach\n",
    "3. **8 Key Metrics**: S, Di, Centrality, Criticality, R, Î”ij, I, SÌ„\n",
    "4. **Scenario Analysis**: Test what-if scenarios\n",
    "\n",
    "---\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Compromise Score (C)**: Individual institution stress (0 = no stress)\n",
    "- **Adjacency Matrix (E)**: Network connections between institutions  \n",
    "- **Systemic Risk (S)**: Aggregate risk = individual stress + network structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization defaults\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data\n",
    "\n",
    "We'll use a 5-institution financial network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load institution data\n",
    "institutions = pd.read_csv('matrix_metrics_app/data/sample_data/das_2016_institutions.csv')\n",
    "print(\"ðŸ“Š Institution Data:\")\n",
    "print(institutions)\n",
    "\n",
    "# Load adjacency matrix\n",
    "adjacency = pd.read_csv('matrix_metrics_app/data/sample_data/das_2016_adjacency.csv', index_col=0)\n",
    "print(\"\\nðŸ”— Network Adjacency Matrix:\")\n",
    "print(adjacency)\n",
    "\n",
    "# Extract key data\n",
    "institution_names = institutions['Name'].values\n",
    "compromise_scores = institutions['CompromiseScore'].values\n",
    "n_institutions = len(institutions)\n",
    "\n",
    "print(f\"\\nâœ… Loaded {n_institutions} institutions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SystemicRiskCalculator Class\n",
    "\n",
    "**ðŸ‘‰ ACTION REQUIRED**: Copy the `SystemicRiskCalculator` class from `matrix_metrics_app/src/core/systemic_risk.py` into the cell below.\n",
    "\n",
    "The class should include all these methods:\n",
    "- `calculate_S()` - Systemic Risk Score\n",
    "- `calculate_risk_decomposition()` - Risk Decomposition  \n",
    "- `calculate_centrality()` - Centrality\n",
    "- `calculate_criticality()` - Criticality\n",
    "- `calculate_fragility()` - Fragility\n",
    "- `calculate_normalized_score()` - Normalized Score\n",
    "- `calculate_risk_increment()` - Risk Increment\n",
    "- `calculate_cross_risk()` - Cross-Risk Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy SystemicRiskCalculator class here from:\n",
    "# matrix_metrics_app/src/core/systemic_risk.py\n",
    "#\n",
    "# For now, run this simplified version:\n",
    "\n",
    "class SystemicRiskCalculator:\n",
    "    def __init__(self, compromise_vector, adjacency_matrix):\n",
    "        self.C = np.array(compromise_vector)\n",
    "        self.E = np.array(adjacency_matrix)\n",
    "        self.n = len(self.C)\n",
    "        self._s_score = None\n",
    "        self._gradient_s = None\n",
    "    \n",
    "    @property\n",
    "    def S(self):\n",
    "        if self._s_score is None:\n",
    "            self._s_score = self.calculate_S()\n",
    "        return self._s_score\n",
    "    \n",
    "    def calculate_S(self):\n",
    "        c_col = self.C.reshape(-1, 1)\n",
    "        s_squared = c_col.T @ self.E @ c_col\n",
    "        return np.sqrt(s_squared.item())\n",
    "    \n",
    "    def _get_gradient(self):\n",
    "        if self._gradient_s is None:\n",
    "            if self.S == 0:\n",
    "                self._gradient_s = np.zeros(self.n)\n",
    "            else:\n",
    "                self._gradient_s = (self.E + self.E.T) @ self.C / (2 * self.S)\n",
    "        return self._gradient_s\n",
    "    \n",
    "    def calculate_risk_decomposition(self):\n",
    "        gradient_s = self._get_gradient()\n",
    "        return self.C * gradient_s\n",
    "    \n",
    "    def calculate_centrality(self):\n",
    "        degrees = np.sum(np.abs(self.E), axis=1) + np.sum(np.abs(self.E), axis=0)\n",
    "        if np.sum(degrees) > 0:\n",
    "            return degrees / np.sum(degrees)\n",
    "        return np.zeros(self.n)\n",
    "    \n",
    "    def calculate_criticality(self):\n",
    "        centrality = self.calculate_centrality()\n",
    "        if np.sum(self.C) > 0:\n",
    "            normalized_C = self.C / np.sum(self.C)\n",
    "            return centrality * normalized_C\n",
    "        return np.zeros(self.n)\n",
    "    \n",
    "    def calculate_fragility(self):\n",
    "        degrees = np.sum(np.abs(self.E), axis=1) + np.sum(np.abs(self.E), axis=0)\n",
    "        total_degree = np.sum(degrees)\n",
    "        if total_degree == 0:\n",
    "            return 0.0\n",
    "        degree_shares = degrees / total_degree\n",
    "        return np.sum(degree_shares ** 2)\n",
    "    \n",
    "    def calculate_normalized_score(self):\n",
    "        norm_c = np.linalg.norm(self.C)\n",
    "        if norm_c == 0:\n",
    "            return 0.0\n",
    "        return self.S / norm_c\n",
    "    \n",
    "    def calculate_risk_increment(self):\n",
    "        Di = self.calculate_risk_decomposition()\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            increment = np.where(self.C != 0, Di / self.C, 0)\n",
    "        return increment\n",
    "    \n",
    "    def calculate_cross_risk(self):\n",
    "        if self.S == 0:\n",
    "            return np.zeros((self.n, self.n))\n",
    "        gradient = self._get_gradient()\n",
    "        term1 = (self.E + self.E.T) / (2 * self.S)\n",
    "        term2 = np.outer(gradient, gradient) / self.S\n",
    "        hessian = term1 - term2\n",
    "        return hessian\n",
    "\n",
    "print(\"âœ… SystemicRiskCalculator class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize calculator\n",
    "calc = SystemicRiskCalculator(compromise_scores, adjacency.values)\n",
    "\n",
    "# Calculate all metrics\n",
    "S = calc.S\n",
    "Di = calc.calculate_risk_decomposition()\n",
    "centrality = calc.calculate_centrality()\n",
    "criticality = calc.calculate_criticality()\n",
    "R = calc.calculate_fragility()\n",
    "S_bar = calc.calculate_normalized_score()\n",
    "I = calc.calculate_risk_increment()\n",
    "Delta = calc.calculate_cross_risk()\n",
    "\n",
    "# Create summary\n",
    "results = pd.DataFrame({\n",
    "    'Institution': institution_names,\n",
    "    'Compromise Score': compromise_scores,\n",
    "    'Risk Decomposition (Di)': Di,\n",
    "    'Centrality': centrality,\n",
    "    'Criticality': criticality,\n",
    "    'Risk Increment (I)': I\n",
    "})\n",
    "\n",
    "print(\"ðŸ“Š SYSTEMIC RISK METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸŽ¯ Systemic Risk Score (S): {S:.4f}\")\n",
    "print(f\"ðŸ”— Network Fragility (R): {R:.4f}\")\n",
    "print(f\"ðŸ“ˆ Normalized Risk Score (SÌ„): {S_bar:.4f}\")\n",
    "print(f\"\\nðŸ’¡ Interpretation: SÌ„ = {S_bar:.2f}\", end=\"\")\n",
    "if S_bar > 1:\n",
    "    print(\" > 1: Network AMPLIFIES risk\")\n",
    "elif S_bar < 1:\n",
    "    print(\" < 1: Network DAMPENS risk\")\n",
    "else:\n",
    "    print(\" = 1: Network neutral\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nInstitution-Level Metrics:\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk Decomposition\n",
    "fig = px.bar(results.sort_values('Risk Decomposition (Di)', ascending=False),\n",
    "             x='Institution', y='Risk Decomposition (Di)',\n",
    "             title='Risk Decomposition by Institution',\n",
    "             color='Risk Decomposition (Di)',\n",
    "             color_continuous_scale='Reds')\n",
    "fig.show()\n",
    "\n",
    "print(f\"ðŸ’¡ Sum of all Di = {np.sum(Di):.4f} = S = {S:.4f} âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scenario Analysis: What-If\n",
    "\n",
    "What happens if we double Bank 5's stress?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scenario\n",
    "modified_scores = compromise_scores.copy()\n",
    "modified_scores[4] = modified_scores[4] * 2  # Double Bank 5's stress\n",
    "\n",
    "# Recalculate\n",
    "calc_mod = SystemicRiskCalculator(modified_scores, adjacency.values)\n",
    "S_mod = calc_mod.S\n",
    "Di_mod = calc_mod.calculate_risk_decomposition()\n",
    "\n",
    "# Compare\n",
    "comparison = pd.DataFrame({\n",
    "    'Institution': institution_names,\n",
    "    'Original Di': Di,\n",
    "    'Modified Di': Di_mod,\n",
    "    'Change': Di_mod - Di\n",
    "})\n",
    "\n",
    "print(\"ðŸ“Š SCENARIO: Double Bank 5's stress\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original S: {S:.4f}\")\n",
    "print(f\"Modified S: {S_mod:.4f}\")\n",
    "print(f\"Change: {S_mod - S:.4f} ({(S_mod-S)/S*100:.1f}%)\\n\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig = px.bar(comparison, x='Institution', y=['Original Di', 'Modified Di'],\n",
    "             title='Risk Decomposition: Original vs Modified', barmode='group')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "**What We Learned:**\n",
    "1. **S = âˆš(C'EC)**: Combines individual stress with network structure\n",
    "2. **Di (Risk Decomposition)**: Identifies which institutions contribute most (Sum = S)\n",
    "3. **Centrality vs Criticality**: Position vs position Ã— stress\n",
    "4. **R (Fragility)**: Higher = more concentrated, vulnerable network\n",
    "5. **SÌ„ (Normalized Score)**: > 1 = network amplifies, < 1 = dampens\n",
    "6. **Scenario Analysis**: Test what-if scenarios easily\n",
    "\n",
    "**Try It Yourself!**\n",
    "- Change different institutions' scores\n",
    "- Remove institutions\n",
    "- Load your own data\n",
    "\n",
    "**References:**\n",
    "- Das, S.R. (2016). Matrix Metrics: Network-Based Systemic Risk Scoring\n",
    "- GitHub: https://github.com/1sbsharma/SystemicRisk_NetworkMatrix_RiskMetrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
